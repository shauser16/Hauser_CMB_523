---
title: "Machine Learning to Classify Breast Tissue Leisions as Benign or Malignant"
output: html_notebook
author: Stephen Hauser and Prof Fisk
---

Today, we are going to work with some real data and machine to get used to the process of using packages without *really* knowing what the package is doing. 


UC Irvine provides a lot of data for oncology machine learning training. 
You are going to write a random forest classifier that will learn how to classify breast leisions as benign or malignant.

I've labelled the columns of the data for you. There are 32 columns, 30 of which contain data that we are going to use to train our model. One of the remaining columns (the first) is an ID that identifies the sample uniquely and the other (the second) contains the answer (known as the label) that we are going to train our machine learning algorithm to classify. 

We are going to use the caret and randomForest packages to do so.
 
```{r}
if(!"caret"%in%installed.packages()){install.packages("caret", dependencies = TRUE)}
if(!"randomForest"%in%installed.packages()){install.packages("randomForest")}
library(caret)
library(randomForest)
```

Next, lets read in the data.

```{r}
#set FILENAME to be the location of the le on your computer
FILENAME<-"/Users/itbur/Downloads/CMB_523_Assignments/breastFNA.csv" 
all_breast_data<-read.csv(FILENAME,header = T)
```

The above code should load the data in as a variable called "all_breast_data". One common feature of machine learning classifiers is that we need to split the data we have into different pieces called training and testing. The training data is what the computer will use to analyze underlying and complex relationships between the data and the labels. The testing data is what we will use to determine if the machine learning algorithm worked well or not (that is, we leave some data out so we can give the program a test it doesn't know the answer to). 

```{r}
#<write a line of code that prints how many rows are in the data>
nrow(all_breast_data)
```

It is not uncommon to set aside around one-fifth of your data for testing. To make things easy to interpret, you will set aside 100 samples for testing. Split all_breast_data into two variables, one called testDat, containing 100 random rows from all_breast_data, and trainDat, containing the rest.

```{r}
set.seed(42666) #just makes it easier for Dr. Fisk to grade

#YOUR CODE HERE
#there are lots of ways to do this
#if you are having a hard time figuring out what to do
#then I recommend creating a vector of all the numbers between 1
#and the number of rows. Then, I would use the sample() function without replacement to randomly pick 100 row numbers from the aforementioned vector and save it as a new vector.
#Then, test would be testDat<-all_breast_data[new_vector,] and train would be trainDat<-all_breast_data[-new_vector,]
#however you do it is fine
all_rows <- 1:569 #this creates a vector of all number of rows
new_vector <- sample(all_rows,size=100, replace= FALSE) #pulls 100 random rows
testDat<- all_breast_data[new_vector,]
print(testDat) 
trainDat<- all_breast_data[-new_vector,] 
print(trainDat)
```
Great! Now that you've split your data in two, we are going to train our machine learning model! We need to do just a bit of data cleaning first by converting our training label to something called a factor.

QUESTION 1: What is a factor data type in R? 
  A factor data type in R is a way categorize data, like the ability to observe the data frame under categories such as ID, diagnosis,radius, perimeter and others.

```{r}
##NONE OF THIS IS CODE YOU NEED TO CHANGE OR ADD TO, BUT LOOK IT OVER ALL THE SAME

#diagnosis is what we are trying to predict
#B (benign) or M (malignant)
trainDat$Diagnosis<-factor(trainDat$Diagnosis)

#now we are going to train the model
#we are using the randomforest algorithm, which is 'rf' and what is called a 5 fold cross-validation where the algorithm is trained on random subsets of the data and tested to get initial accuracy. 
model<-train(Diagnosis~.,
            data=trainDat,
            method ='rf',
            trControl=trainControl(method = "cv",number = 5)
)

print(model)
```

QUESTION 2: What mtry value had the best accuracy for your model?
  Mtry had the best accuracy value at 31.

mtry is what is called a hyperparameter. It is basically a number that somehow shapes how well the machine learning model does. For random forests, they have a hyperparameter called mtry that decides how many variables are used at each split in the decision tree. 


Now we are going to test our model on data it has never seen before (testDat) and see how well it learned.
```{r}
###AGAIN, NO NEED TO ALTER ANY OF THIS RIGHT NOW

#need to convert the diagnosis into a factor in the test data too
testDat$Diagnosis<-factor(testDat$Diagnosis)

#make the predictions and store them in a new column called prediction
testDat$Prediction<-predict(model, newdata = testDat)

```

Now your variable testDat has a new column with the predictions the machine model made, given the other columns. How did it do? You are going to write code to find out! The "Diagnosis" column has the true answers. The "Prediction" column has the predicted answers. Your task here is to determine for how many of the rows out of 100 they do not match.

```{r}
#YOU DO NEED CODE HERE
#write code to determine how many values in trainDat's Prediction column (accessible with $) do not match the value in it's Diagnosis column (accessible with $)
not_match<-sum(trainDat$Prediction != trainDat$Diagnosis)
print(not_match)
```

Below is code that will tell you the true diagnosis of those that the model misclassified. That is, when it was wrong, what was the right answer.
```{r}
#NO NEED FOR YOU TO ADD CODE HERE
print(summary(testDat[which(trainDat$Diagnosis!=testDat$Prediction),"Diagnosis"]))
```
QUESTION 3: In your results, was there a bias to which kind of leision was misclassified more often? 
  In my results, the NA legion was found to be mismatched 178 times compared to the M legion which was mismatched 13 times and the B lesion which was mismatched 35 times.

QUESTION 4: Do you think that it would be better for the machine learning algorithm to classify benign tumors as malignant or malignant tumors as benign. What other information might you want to make that decision? (Not really a right or wrong questions, but making you think about what your data looks like)
  It would be better to classify malignant tumors as benign in the algorithm because this allows for more research to be done on a patient regardless of whether a tumor is malignant or not. Other information that could be beneficial to the algorithm is to further classify why each tumor was considered malignant or benign, and order them in such a way in the form of severity. 

There are only 2 outcomes the model could decide: B or M.

QUESTION 5: If you or your model just guessed at random (equal chance in this case), what do you expect your accuracy to be?
  It would likely be 50% accurate.


There might be more to it than you thought at first glance. Lets think back to all the data. How many samples in the training data are B and how many are M?
```{r}
#NO NEED TO ADD CODE HERE
print(table(all_breast_data$Diagnosis))
print(table(all_breast_data$Diagnosis)/nrow(all_breast_data))
```
Hmm... around 63% of the data is benign. That means that if you guessed benign every time, no matter what, you would get an accuracy of 63%! Better than the (probably) 50% you guessed above. Indeed, guessing at random (with equal probabilities, like a coin flip) would get you below a 50% accuracy on this data, on average because there is more benign data than malignant data.

Why is this important? In your projects (and in your life!) understanding what we expect to see is key to understanding whether or not what we see is suprising or important. In this case, the dumbest model could achieve 63% accuracy by just always guessing benign. So if you wrote a program that gave you an accuracy of, say, 70%, that would not actually be a very impressive machine learning program!

Think critically about your data whenever you can. Ask "are the characteristics uniformly distributed among members of my data?" or more simply "what do I expect to see and how often do I expect to see it". 


We are going to see if doing any preprocessing of our data helps with the outcome at all. The chances are it probably won't for a random forest, but it is good practice all the same. All the new bit of code below does is change the numbers such that they are centered about a single number and scaled proportionally between 0 and 1. There are reasons this can help not worth getting into today.

```{r}
#NO NEED FOR YOU TO ALTER THIS CODE
#Same thing but with preprocessing
#You can use this block of code again down below.
model2<-train(Diagnosis~.,
            data=trainDat,
            method ='rf',
            preProc=c("center","scale"), #note this line
            trControl=trainControl(method = "cv",number = 5)
)

testDat$Prediction2<-predict(model2, newdata = testDat)
print(summary(testDat[which(testDat$Diagnosis!=testDat$Prediction2),"Diagnosis"]))

```
As expected, the preprocessing didn't help in this case. However, more numerically intensive methods can benefit from this preprocessing. 

Here is where I leave you with one last counter-intutive fact about working with data. 

More isn't always better.

```{r}
# NO NEED FOR YOU TO ALTER THE CODE HERE
#plot the model
ggplot(model2)
```

As the decision tree considers more and more predictors to include (that is, columns to include), its prediction accuracy actually goes down. Collecting more rows of data can usually help this problem, but in biology we are often dealing with lots of columns, few rows. For instance, each location in the human genome could be considered a column. 

A fundamental task and skill of scientific computing and the analysis of biological data is using your biological knowledge and your tech skills to determine what to include and what to exclude from your data in a way that doesn't also just bias the answer to what you want to see.

A delicate balance indeed!


You last task for today is to repeat the process of training the data, predicting the diagnoses, and calcultaint the % correct and which ones were wrong using a different machine learning method.
[A list of available models compatable can be found here.](https://topepo.github.io/caret/available-models.html)
Remember that you are performing a classification task, so don't use a regression model. You may need to install another packackage(s) to get the code to work. But ultimately, your should really have to change the method="rf" parameter from the train function above. Plot the model once it is done training, like I did above with the ggplot function. 


```{r}
### Your code here, including installing and loading (via library) whatever other packages you end up needing to use
### you can copy what I did above--that is ultimately how I wrote and ran the code, too!
### change the method from 'rf' to another classification method.
### don't forget to train the model first, then predict the outcomes, then calculate how many you got write, then plot the model.
### this should be mostly copy-ing and pasting, like you will be doing in your final projects!
modelInfo <- list(label = "AdaBoost.M1",
                  library = c("adabag", "plyr"),
                  loop = function(grid) {     
                    loop <- plyr::ddply(grid, c("coeflearn", "maxdepth"),
                                  function(x) c(mfinal = max(x$mfinal)))
                    submodels <- vector(mode = "list", length = nrow(loop))
                    for(i in seq(along = loop$mfinal)) {
                      index <- which(grid$maxdepth == loop$maxdepth[i] & 
                                       grid$coeflearn == loop$coeflearn[i])
                      trees <- grid[index, "mfinal"] 
                      submodels[[i]] <- data.frame(mfinal = trees[trees != loop$mfinal[i]])
                    }    
                    list(loop = loop, submodels = submodels)
                  },
                  type = c("Classification"),
                  parameters = data.frame(parameter = c('mfinal', 'maxdepth', 'coeflearn'),
                                          class = c("numeric", "numeric", "character"),
                                          label = c('#Trees', 'Max Tree Depth', 'Coefficient Type')),
                  grid = function(x, y, len = NULL, search = "grid") {
                    types <- c("Breiman", "Freund", "Zhu")
                    if(search == "grid") {
                      out <- expand.grid(mfinal = floor((1:len) * 50),
                                         maxdepth = seq(1, len),         
                                         coeflearn = types)
                    } else {
                      out <- data.frame(mfinal = sample(1:100, replace = TRUE, size = len),
                                        maxdepth = sample(1:30, replace = TRUE, size = len),
                                        coeflearn = sample(types, replace = TRUE, size = len))
                    }
                    out
                  },
                  fit = function(x, y, wts, param, lev, last, classProbs, ...) {
                    theDots <- list(...)
                    
                    if(any(names(theDots) == "control")) {
                      theDots$control$maxdepth <- param$maxdepth 
                      ctl <- theDots$control
                      theDots$control <- NULL
                      
                    } else ctl <- rpart::rpart.control(maxdepth = param$maxdepth,
                                                cp=-1,minsplit=0,xval=0) 
                    
                    if (!is.data.frame(x) | inherits(x, "tbl_df"))
                      x <- as.data.frame(x, stringsAsFactors = TRUE)
                    
                    modelArgs <- c(list(formula = as.formula(.outcome ~ .),
                                        data = x,
                                        mfinal = param$mfinal,
                                        coeflearn = as.character(param$coeflearn),              
                                        control = ctl),
                                   theDots)
                    modelArgs$data$.outcome <- y
                    out <- do.call(adabag::boosting, modelArgs)                    
                    out     
                  },
                  predict = function(modelFit, newdata, submodels = NULL) {
                    if (!is.data.frame(newdata) | inherits(newdata, "tbl_df"))
                      newdata <- as.data.frame(newdata, stringsAsFactors = TRUE)
                    ## The predict function requires the outcome! Trick it by
                    ## adding bogus data
                    newdata$.outcome <- factor(rep(modelFit$obsLevels[1], nrow(newdata)), 
                                               levels = modelFit$obsLevels)
                    out <- predict(modelFit, newdata, 
                                   newmfinal = modelFit$tuneValue$mfinal)$class
                    
                    if(!is.null(submodels)) {
                      tmp <- vector(mode = "list", length = length(submodels$mfinal)+1)
                      tmp[[1]] <- out
                      for(i in seq(along = submodels$mfinal)) {
                        tmp[[i+1]] <- predict(modelFit, newdata, 
                                              newmfinal = submodels$mfinal[[i]])$class
                      }
                      out <- tmp
                    }       
                    out  
                  },
                  prob = function(modelFit, newdata, submodels = NULL){
                    if (!is.data.frame(newdata) | inherits(newdata, "tbl_df"))
                      newdata <- as.data.frame(newdata, stringsAsFactors = TRUE)
                    ## The predict function requires the outcome! Trick it by
                    ## adding bogus data
                    newdata$.outcome <- factor(rep(modelFit$obsLevels[1], nrow(newdata)), 
                                               levels = modelFit$obsLevels)
                    out <- predict(modelFit, newdata)$prob
                    colnames(out) <- modelFit$obsLevels
                    
                    if(!is.null(submodels)) {
                      tmp <- vector(mode = "list", length = length(submodels$mfinal)+1)
                      tmp[[1]] <- out
                      for(i in seq(along = submodels$mfinal)) {
                        tmp[[i+1]] <- predict(modelFit, newdata,  
                                              newmfinal = submodels$mfinal[[i]])$prob
                        colnames(tmp[[i+1]]) <- modelFit$obsLevels
                      }
                      out <- lapply(tmp, as.data.frame)
                    }
                    
                    out 
                  },
                  levels = function(x) x$obsLevels,
                  varImp = function(object, ...){
                    imps <- data.frame(Overall = object$importance)
                    rownames(imps) <- names(object$importance)
                    imps
                  },
                  predictors = function(x, ...) names(x$importance)[x$importance != 0],
                  tags = c("Tree-Based Model", "Ensemble Model", "Boosting", 
                           "Implicit Feature Selection", "Handle Missing Predictor Data"),
                  sort = function(x) x[order(x$mfinal, x$maxdepth),])

#Utilizing ADABoost.M1 model
if(!"caret"%in%installed.packages()){install.packages("caret", dependencies = TRUE)}
if(!"adabag"%in%installed.packages()){install.packages("adabag")}
library(caret)
library(adabag)
library(plyr)
#Set file name
FILENAME<-"/Users/itbur/Downloads/CMB_523_Assignments/breastFNA.csv" 
all_breast_data<-read.csv(FILENAME,header = T)
#printing the number of rows
nrow(all_breast_data)
#creating a vector of all rows 
set.seed(42666)
all_rows <- 1:569 #this creates a vector of all number of rows
new_vector <- sample(all_rows,size=100, replace= FALSE) #pulls 100 random rows
testDat<- all_breast_data[new_vector,]
print(testDat) 
trainDat<- all_breast_data[-new_vector,] 
print(trainDat)
#predict and train model
trainDat$Diagnosis<-factor(trainDat$Diagnosis)
#now train it
model<-train(Diagnosis~.,
            data=trainDat,
            method ='rf',
            trControl=trainControl(method = "cv",number = 5)
)

print(model)
#convert diagnosis into a factor
testDat$Diagnosis<-factor(testDat$Diagnosis)
#make new prediction and store in prediction into columns
testDat$Prediction<-predict(model, newdata = testDat)
# how many values in diagnosis and prediction columns do not match
not_match<-sum(trainDat$Prediction != trainDat$Diagnosis)
print(not_match)
print(summary(testDat[which(trainDat$Diagnosis!=testDat$Prediction),"Diagnosis"]))
print(table(all_breast_data$Diagnosis))
print(table(all_breast_data$Diagnosis)/nrow(all_breast_data))
#preprocessing
model2<-train(Diagnosis~.,
            data=trainDat,
            method ='rf',
            preProc=c("center","scale"), #note this line
            trControl=trainControl(method = "cv",number = 5)
)

testDat$Prediction2<-predict(model2, newdata = testDat)
print(summary(testDat[which(testDat$Diagnosis!=testDat$Prediction2),"Diagnosis"]))
#plot the model
ggplot(model2)
```

QUESTION 6: What did you think of this assignment?
  This assignment taught me an effective way to organize, predict, store, and visualize large quantities of data. Running this code under different model allowed me to see the how data can be organized in a data frame in my own chosen way. 
